{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Churn Prediction\n\n## Introduction <a name=\"introduction\"></a>\n\nCustomer churn, often referred to as churn rate, represents the scenario where a customer discontinues their relationship with a company, quit using a particular service, or no longer purchases a specific product. It is a significant business metric, primarily because the acquisition of new customers entails considerably higher costs compared to retaining existing ones.\n\nPredicting customer churn is a strategic tool that empowers businesses to proactively identify potential risks associated with customer churn, allowing for timely intervention. By identifying these risks, they can take actions such as personalized marketing campaigns, extending offers, or improving customer service to ensure their continued loyalty and engagement with the company.\n\nCustomer churn prediction is a application that relies heavily on machine learning algorithms like logistic regression and random forests. While these ML techniques good at identifying customers likely to churn, statistical tools such as survival analysis have appeared as important components of this predictive process. While algorithms like logistic regression prediction offer insights into which customers are more prone to leaving, survival analysis goes a step further by not only predicting the likelihood of customer churn but also predicting when such events might occur. Hence, some argue that the survival analysis is essential for achieving more accurate and actionable insights in customer retention strategies. See : https://towardsdatascience.com/better-churn-prediction-part-2-5a1086fd3f51\n\nIn this kernel, I want to explore machine learning and survival analysis techniques to construct a robust customer churn prediction model. By doing so, I can evaluate their respective performance and determine which models show better predictive capabilities in tackling the complex challenge of customer churn. Following are the model I will be using, \n\n* Machine Learning Based\n    * Logistic Regression\n    * Random Forest\n\n* Survival Models\n    * COX\n    * Survival Forest\n\n\nI choose the Telco Customer Churn data stet available in Kaggle. The data set includes information about:\n\n* Customers who left within the last month – the column is called Churn\n* Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n* Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n* Demographic info about customers – gender, age range, and if they have partners and dependents","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Load the libraries and the data","metadata":{}},{"cell_type":"code","source":"# Common libraries for data analysis\nimport numpy as np\nimport pandas as pd\n\n# Libraries for graphs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix, mutual_info_score, recall_score, accuracy_score, precision_score, f1_score\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.inspection import permutation_importance\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:42.593681Z","iopub.execute_input":"2023-09-15T03:16:42.594117Z","iopub.status.idle":"2023-09-15T03:16:43.890082Z","shell.execute_reply.started":"2023-09-15T03:16:42.594075Z","shell.execute_reply":"2023-09-15T03:16:43.888862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the data\ndata=pd.read_csv(\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndata.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:43.891991Z","iopub.execute_input":"2023-09-15T03:16:43.892775Z","iopub.status.idle":"2023-09-15T03:16:43.972280Z","shell.execute_reply.started":"2023-09-15T03:16:43.892697Z","shell.execute_reply":"2023-09-15T03:16:43.971135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shape of the data\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:43.974044Z","iopub.execute_input":"2023-09-15T03:16:43.975026Z","iopub.status.idle":"2023-09-15T03:16:43.983637Z","shell.execute_reply.started":"2023-09-15T03:16:43.974993Z","shell.execute_reply":"2023-09-15T03:16:43.982592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# type of each column\ndata.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:43.987031Z","iopub.execute_input":"2023-09-15T03:16:43.987476Z","iopub.status.idle":"2023-09-15T03:16:44.000084Z","shell.execute_reply.started":"2023-09-15T03:16:43.987444Z","shell.execute_reply":"2023-09-15T03:16:43.998850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unique values of each column\nfor col in data:\n    print(col, \":\" ,data[col].unique())","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.002130Z","iopub.execute_input":"2023-09-15T03:16:44.002589Z","iopub.status.idle":"2023-09-15T03:16:44.038829Z","shell.execute_reply.started":"2023-09-15T03:16:44.002558Z","shell.execute_reply":"2023-09-15T03:16:44.037886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing <a name=\"Preprocessing\"></a>","metadata":{}},{"cell_type":"code","source":"# check for duplicate customers\ndata['customerID'].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.040220Z","iopub.execute_input":"2023-09-15T03:16:44.041144Z","iopub.status.idle":"2023-09-15T03:16:44.050386Z","shell.execute_reply.started":"2023-09-15T03:16:44.041108Z","shell.execute_reply":"2023-09-15T03:16:44.049188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total charge appers as a string features that contains empty strings, so let's remove the rows with empty strings\ndata_df = data.loc[data['TotalCharges']!=' ']","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.051461Z","iopub.execute_input":"2023-09-15T03:16:44.051836Z","iopub.status.idle":"2023-09-15T03:16:44.066857Z","shell.execute_reply.started":"2023-09-15T03:16:44.051804Z","shell.execute_reply":"2023-09-15T03:16:44.065617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove the customer ID\ndata_df=data_df.drop(['customerID'],axis=1)\n\n# convert TotalCharges to float\ndata_df = data_df.astype({\"TotalCharges\": float})","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.068270Z","iopub.execute_input":"2023-09-15T03:16:44.069197Z","iopub.status.idle":"2023-09-15T03:16:44.091030Z","shell.execute_reply.started":"2023-09-15T03:16:44.069135Z","shell.execute_reply":"2023-09-15T03:16:44.090016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for other missing value percentage\n(data_df.isnull().sum() / len(data_df)).sort_values(ascending=False) * 100","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.092263Z","iopub.execute_input":"2023-09-15T03:16:44.092567Z","iopub.status.idle":"2023-09-15T03:16:44.120184Z","shell.execute_reply.started":"2023-09-15T03:16:44.092541Z","shell.execute_reply":"2023-09-15T03:16:44.119121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis <a name=\"EDA\"></a>","metadata":{}},{"cell_type":"code","source":"# check the class balance\nprint(\"Value counts of\", data['Churn'].value_counts())\nsns.histplot(data=data_df, x=\"Churn\", hue=\"Churn\", palette = \"Set1\")\nplt.title('Churn of Customers')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.121664Z","iopub.execute_input":"2023-09-15T03:16:44.122816Z","iopub.status.idle":"2023-09-15T03:16:44.510873Z","shell.execute_reply.started":"2023-09-15T03:16:44.122774Z","shell.execute_reply":"2023-09-15T03:16:44.509718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the plot describes, many of the data points belong to the Customers that have not left the organization which is around 73%. We have to keep in mind this 'Class Imbalance' when selecting training data.","metadata":{}},{"cell_type":"code","source":"# gender based churn\nsns.histplot(binwidth=1,\n            x='gender',\n            hue='Churn',\n            data=data_df,\n            stat=\"count\",\n            multiple=\"dodge\", palette = \"Set1\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.512245Z","iopub.execute_input":"2023-09-15T03:16:44.513500Z","iopub.status.idle":"2023-09-15T03:16:44.853536Z","shell.execute_reply.started":"2023-09-15T03:16:44.513458Z","shell.execute_reply":"2023-09-15T03:16:44.852293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df['gender'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.855078Z","iopub.execute_input":"2023-09-15T03:16:44.855544Z","iopub.status.idle":"2023-09-15T03:16:44.866799Z","shell.execute_reply.started":"2023-09-15T03:16:44.855501Z","shell.execute_reply":"2023-09-15T03:16:44.865477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no significant differences in the gender for each two classes. The female and male Churn values are closer.","metadata":{}},{"cell_type":"code","source":"# relationship between the tenure and the average total charges\nave_tot_ch=data_df.groupby(['tenure', 'Churn'])['TotalCharges'].mean().reset_index(name='AveTotalCharges')\nsns.scatterplot(data=ave_tot_ch, x= 'tenure', y='AveTotalCharges', hue='Churn')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:44.868120Z","iopub.execute_input":"2023-09-15T03:16:44.868747Z","iopub.status.idle":"2023-09-15T03:16:45.303092Z","shell.execute_reply.started":"2023-09-15T03:16:44.868712Z","shell.execute_reply":"2023-09-15T03:16:45.302013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# relationship between the tenure and the average total charges\nave_monthly_ch=data_df.groupby(['tenure', 'Churn'])['MonthlyCharges'].mean().reset_index(name='AverageMonthlyCharges')\nsns.scatterplot(data=ave_monthly_ch, x= 'tenure', y='AverageMonthlyCharges', hue='Churn')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:45.309171Z","iopub.execute_input":"2023-09-15T03:16:45.309573Z","iopub.status.idle":"2023-09-15T03:16:45.739905Z","shell.execute_reply.started":"2023-09-15T03:16:45.309542Z","shell.execute_reply":"2023-09-15T03:16:45.738560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The relationship between the average monthly charges vs. tenure is a linear relationship. The plot indicates the higher the tenure, there is an increase in the average monthly charges. It also indicates that the customers who have churned paid a higher amount for the monthly charges than the customers who have not churned yet. We observed a similar trend in the previous plot, which shows the relationship between total charges vs. tenure with regard to churn.","metadata":{}},{"cell_type":"code","source":"# relationship between the tenure and the average MonthlyCharges based on gender\nave_monthly_gen=data_df.groupby(['tenure', 'gender'])['MonthlyCharges'].mean().reset_index(name='AverageMonthlyCharges')\nsns.scatterplot(data=ave_monthly_gen, x= 'tenure', y='AverageMonthlyCharges', hue='gender', palette = \"Set1\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:45.741504Z","iopub.execute_input":"2023-09-15T03:16:45.741861Z","iopub.status.idle":"2023-09-15T03:16:46.175222Z","shell.execute_reply.started":"2023-09-15T03:16:45.741832Z","shell.execute_reply":"2023-09-15T03:16:46.174089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, I will convert some binary categorical variables to binary values. Later, I will use pd.get_dummies() to apply one hot encoding to the rest of the categorical variables. ","metadata":{}},{"cell_type":"code","source":"# Convert Yes and No values to 0 and 1\ndata_df['Churn']=data_df.Churn.map(dict(Yes=1, No=0))\n#data_df['gender']=data_df.Partner.map(dict(Male=1, Female=0))\ndata_df['Partner']=data_df.Partner.map(dict(Yes=1, No=0))\ndata_df['Dependents']=data_df.Dependents.map(dict(Yes=1, No=0))\ndata_df['PhoneService']=data_df.PhoneService.map(dict(Yes=1, No=0))\ndata_df['PaperlessBilling']=data_df.PaperlessBilling.map(dict(Yes=1, No=0))","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:46.176810Z","iopub.execute_input":"2023-09-15T03:16:46.177857Z","iopub.status.idle":"2023-09-15T03:16:46.196097Z","shell.execute_reply.started":"2023-09-15T03:16:46.177820Z","shell.execute_reply":"2023-09-15T03:16:46.194759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:46.197335Z","iopub.execute_input":"2023-09-15T03:16:46.197746Z","iopub.status.idle":"2023-09-15T03:16:46.231883Z","shell.execute_reply.started":"2023-09-15T03:16:46.197714Z","shell.execute_reply":"2023-09-15T03:16:46.230588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:46.233326Z","iopub.execute_input":"2023-09-15T03:16:46.234103Z","iopub.status.idle":"2023-09-15T03:16:46.248085Z","shell.execute_reply.started":"2023-09-15T03:16:46.234072Z","shell.execute_reply":"2023-09-15T03:16:46.247096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Analysis\n\nCorrelation Analysis measures the correlation between the variables or features. In general, we expect no correlation among the independent variables and some correlation between the dependent variable and each independent variable. ","metadata":{}},{"cell_type":"code","source":"# check correlation between the target and the features\nconti_df= data_df.select_dtypes(exclude=[object])\n\ncorr_with_tot_count = conti_df.corr()[\"Churn\"].sort_values(ascending=False)\n\nplt.figure(figsize=(8,6))\ncorr_with_tot_count.drop(\"Churn\").plot.bar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:46.249574Z","iopub.execute_input":"2023-09-15T03:16:46.249977Z","iopub.status.idle":"2023-09-15T03:16:46.542740Z","shell.execute_reply.started":"2023-09-15T03:16:46.249948Z","shell.execute_reply":"2023-09-15T03:16:46.541642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for correlation among the features\n\n#pd.set_option('precision',2)\nplt.figure(figsize=(6,6))\n\nsns.heatmap(conti_df.drop(['Churn'],axis=1).corr(), square=True, annot=True,)\nplt.suptitle(\"Pearson Correlation Heatmap\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:46.544363Z","iopub.execute_input":"2023-09-15T03:16:46.544804Z","iopub.status.idle":"2023-09-15T03:16:47.080276Z","shell.execute_reply.started":"2023-09-15T03:16:46.544765Z","shell.execute_reply":"2023-09-15T03:16:47.079369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create correlation matrix\ncorr_matrix = conti_df.drop(['Churn'],axis=1).corr().abs()\n\n# select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n\n# select one feature from the highly correlated features, threshold 0.7\nto_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n\nprint(to_drop)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.081295Z","iopub.execute_input":"2023-09-15T03:16:47.081662Z","iopub.status.idle":"2023-09-15T03:16:47.094641Z","shell.execute_reply.started":"2023-09-15T03:16:47.081632Z","shell.execute_reply":"2023-09-15T03:16:47.093713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on the correlation analysis I will drop the following features from the data\ndf=data_df.drop(['TotalCharges', 'PhoneService'],axis=1)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.095879Z","iopub.execute_input":"2023-09-15T03:16:47.096865Z","iopub.status.idle":"2023-09-15T03:16:47.130587Z","shell.execute_reply.started":"2023-09-15T03:16:47.096833Z","shell.execute_reply":"2023-09-15T03:16:47.129477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mutual Information\n\nMutual information (MI) measures the degree of dependence or statistical association between two random variables. In machine learning, it is  used for feature selection, feature engineering, and measuring the dependence between variables. We can use this to determine the degree of dependency between the dependent variable and each of the independent variables. Mutual information is a non-negative value. A higher mutual information indicates a stronger relationship or dependency between the variables, while a lower value suggests less dependency. \n\nBy looking at the MI scores we can eliminate the independent variables that have a lower degree of dependence on the target variable.  ","metadata":{}},{"cell_type":"code","source":"# returns the MI scores\ndef compute_mutual_information(categorical_serie):\n    return mutual_info_score(categorical_serie, df.Churn)\n\n# select categorial variables excluding Churn\ncat_vars = df.select_dtypes(include=object)\n\n# compute the MI score between each categorical variable and the target\nfeature_importance_df = cat_vars.apply(compute_mutual_information).sort_values(ascending=False)\n\n# visualize feature importance\nplt.bar(feature_importance_df.index, height = feature_importance_df.values)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.132040Z","iopub.execute_input":"2023-09-15T03:16:47.132419Z","iopub.status.idle":"2023-09-15T03:16:47.601059Z","shell.execute_reply.started":"2023-09-15T03:16:47.132388Z","shell.execute_reply":"2023-09-15T03:16:47.599263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the least important features that has low MI scores, the threshold hee is arbitarary\nprint(feature_importance_df)\ntodrop = feature_importance_df[feature_importance_df < 0.06]\ntodrop.index.values","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.603192Z","iopub.execute_input":"2023-09-15T03:16:47.603756Z","iopub.status.idle":"2023-09-15T03:16:47.619200Z","shell.execute_reply.started":"2023-09-15T03:16:47.603653Z","shell.execute_reply":"2023-09-15T03:16:47.617936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features=df.drop(todrop.index.values,axis=1)\nselected_features.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.620896Z","iopub.execute_input":"2023-09-15T03:16:47.621369Z","iopub.status.idle":"2023-09-15T03:16:47.648188Z","shell.execute_reply.started":"2023-09-15T03:16:47.621330Z","shell.execute_reply":"2023-09-15T03:16:47.647219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.650613Z","iopub.execute_input":"2023-09-15T03:16:47.650964Z","iopub.status.idle":"2023-09-15T03:16:47.658127Z","shell.execute_reply.started":"2023-09-15T03:16:47.650935Z","shell.execute_reply":"2023-09-15T03:16:47.657192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Variance Inflation Factor\n\n\nVariance Inflation Factor (VIF) is used to detect the multicollinearity among independent variables within a regression model. VIF quantifies the degree to which the variance of estimated coefficients is inflated due to multicollinearity. This inflation is evaluated with respect to R-squared.\n\n* VIF values above 4 indicate a potential issue that should be explored further.\n* When VIF values are higher than 10, it indicates a more severe case of multicollinearity that requires corrections.","metadata":{}},{"cell_type":"code","source":"def cal_VIF(X):\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] =  X.columns\n    # calculating VIF for each feature\n    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                              for i in range(len(X.columns))] \n    return vif_data\n\n# convert all the categorical data to numerical applying one hot encoding\ninput_df = pd.get_dummies(selected_features.drop(['Churn'],axis=1), dtype=float, drop_first = True)\ncal_VIF(input_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.659661Z","iopub.execute_input":"2023-09-15T03:16:47.659982Z","iopub.status.idle":"2023-09-15T03:16:47.808003Z","shell.execute_reply.started":"2023-09-15T03:16:47.659953Z","shell.execute_reply":"2023-09-15T03:16:47.806868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# based on the higher VIF, I will drop the following features from the data\ninput_df_new = pd.get_dummies(selected_features.drop(['Churn', 'OnlineSecurity', 'TechSupport'],axis=1), dtype=float, drop_first = True)\ncal_VIF(input_df_new)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.809745Z","iopub.execute_input":"2023-09-15T03:16:47.810512Z","iopub.status.idle":"2023-09-15T03:16:47.889022Z","shell.execute_reply.started":"2023-09-15T03:16:47.810470Z","shell.execute_reply":"2023-09-15T03:16:47.887462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select the final set of data and seperate the target and independent variables to y and X\nall_features_df = pd.get_dummies(selected_features[['SeniorCitizen', 'Partner', 'Dependents', 'PaperlessBilling','MonthlyCharges', 'Contract','tenure', 'Churn']],dtype=int, drop_first=True)\nX = all_features_df.drop(['Churn'],axis=1)\ny = all_features_df['Churn']","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.891266Z","iopub.execute_input":"2023-09-15T03:16:47.892147Z","iopub.status.idle":"2023-09-15T03:16:47.916365Z","shell.execute_reply.started":"2023-09-15T03:16:47.892084Z","shell.execute_reply":"2023-09-15T03:16:47.914489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.918332Z","iopub.execute_input":"2023-09-15T03:16:47.919057Z","iopub.status.idle":"2023-09-15T03:16:47.940733Z","shell.execute_reply.started":"2023-09-15T03:16:47.919015Z","shell.execute_reply":"2023-09-15T03:16:47.939567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scaling the data & seperating the train/test data","metadata":{}},{"cell_type":"code","source":"# create the standard scaler \nscaler = StandardScaler()\n# transform data\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns= X.columns)\nX_scaled.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.942596Z","iopub.execute_input":"2023-09-15T03:16:47.943288Z","iopub.status.idle":"2023-09-15T03:16:47.978710Z","shell.execute_reply.started":"2023-09-15T03:16:47.943248Z","shell.execute_reply":"2023-09-15T03:16:47.977579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train/ Test Split using stratify sampling to solve the class imbalance problem\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify = y, test_size=0.2, random_state=33)\n\nprint(f'Percentage of churn in train: {round(100 * (y_train[y_train == 1].shape[0] / len(y_train)),2)} %')\nprint(f'Percentage of churn in test: {round(100 * (y_test[y_test == 1].shape[0] / len(y_test)),2)} %')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:47.980433Z","iopub.execute_input":"2023-09-15T03:16:47.981105Z","iopub.status.idle":"2023-09-15T03:16:48.002738Z","shell.execute_reply.started":"2023-09-15T03:16:47.981055Z","shell.execute_reply":"2023-09-15T03:16:48.001597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\n\nThe first model I will use is the Logistic Regression model. It is a popular supervised learning algorithm mainly applied to binary classification problems. This model calculates the probability of a customer belonging to one of two classes, with outcomes constrained to the range of $[0, 1]$. Logistic regression extends the principles of linear regression but tailors them to probabilistically assess relationships between the target variable, \"Churn,\" and other features. By doing so, it classifies customers into appropriate categories based on these calculated probabilities.","metadata":{}},{"cell_type":"code","source":"# fit the model and predict\nmodel_lr = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred = model_lr.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:48.004548Z","iopub.execute_input":"2023-09-15T03:16:48.005268Z","iopub.status.idle":"2023-09-15T03:16:48.038913Z","shell.execute_reply.started":"2023-09-15T03:16:48.005225Z","shell.execute_reply":"2023-09-15T03:16:48.037328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_lr.predict_proba(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:48.040781Z","iopub.execute_input":"2023-09-15T03:16:48.041472Z","iopub.status.idle":"2023-09-15T03:16:48.046377Z","shell.execute_reply.started":"2023-09-15T03:16:48.041434Z","shell.execute_reply":"2023-09-15T03:16:48.045281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# returns the model performance in terms of accuracy, classification report and the confusion matrix\ndef model_performance(y_test,y_pred, method):\n    \n    # calculate the accruacy of the model\n    print(\"Accuracy score of the model\", accuracy_score(y_test,y_pred))\n    print(\"Classification report \\n\")\n    \n    #generate the classification report\n    print(classification_report(y_test,y_pred))\n    \n    #generate the confusion matrix\n    fig = plt.figure(figsize = (4,4))\n    ax = fig.gca()\n    cnf_matrix_log = confusion_matrix(y_test, y_pred)\n    sns.heatmap(pd.DataFrame(cnf_matrix_log), annot=True,cmap=\"Reds\" , fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix: '+  method + '\\n', y=1.1)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:48.048148Z","iopub.execute_input":"2023-09-15T03:16:48.048869Z","iopub.status.idle":"2023-09-15T03:16:48.064727Z","shell.execute_reply.started":"2023-09-15T03:16:48.048825Z","shell.execute_reply":"2023-09-15T03:16:48.063130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets measure the model performance\nmodel_performance(y_test,y_pred, 'Logistic Regression')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:48.067031Z","iopub.execute_input":"2023-09-15T03:16:48.067961Z","iopub.status.idle":"2023-09-15T03:16:48.657027Z","shell.execute_reply.started":"2023-09-15T03:16:48.067901Z","shell.execute_reply":"2023-09-15T03:16:48.655866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The logistic regression model has attained 80% accuracy. There are more False negatives than False positives indicating high precision and low recall for overall model. For this context, I think we would be aiming for a fewer false positive, so that we can avoid a potential customer churn.","metadata":{}},{"cell_type":"code","source":"    labels=['Churn','Not Churn']\n    p = model_lr.predict_proba(X_test)\n    if len(model_lr.classes_)!=2:\n        raise ValueError('A binary class problem is required')\n    if model_lr.classes_[1] == 1:\n        pos_p = p[:,1]\n    elif model_lr.classes_[0] == 1:\n        pos_p = p[:,0]\n    \n    prob_df = pd.DataFrame({'probPos':pos_p, 'target': y_test})\n\n    plt.hist(prob_df[prob_df.target==1].probPos, density=True, bins=25,alpha=.5, color='green',  label=labels[0])\n    plt.hist(prob_df[prob_df.target==0].probPos, density=True, bins=25,alpha=.5, color='red', label=labels[1])\n    plt.axvline(.5, color='blue', linestyle='--', label='Boundary')\n    plt.xlim([0,1])\n    plt.title('Distributions of Predictions', size=13)\n    plt.xlabel('Positive Probability (predicted)', size=10)\n    plt.ylabel('Samples (normalized scale)', size=10)\n    plt.legend(loc=\"upper right\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:48.658478Z","iopub.execute_input":"2023-09-15T03:16:48.658795Z","iopub.status.idle":"2023-09-15T03:16:49.193935Z","shell.execute_reply.started":"2023-09-15T03:16:48.658767Z","shell.execute_reply":"2023-09-15T03:16:49.192765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply cross validation to generate more generic result\ncv_results = cross_validate(model_lr, X_train, y_train, cv=5)\nscores = cv_results[\"test_score\"]\nprint(\"The mean cross-validation accuracy for Logistic Regression is: \"\n    f\"{scores.mean():.3f} ± {scores.std():.3f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:49.195510Z","iopub.execute_input":"2023-09-15T03:16:49.195859Z","iopub.status.idle":"2023-09-15T03:16:49.339286Z","shell.execute_reply.started":"2023-09-15T03:16:49.195829Z","shell.execute_reply":"2023-09-15T03:16:49.337718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply the grid search for hyper parameter tunning \ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}\nlogreg_cv=GridSearchCV(model_lr,grid,cv=10)\nlogreg_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:49.341729Z","iopub.execute_input":"2023-09-15T03:16:49.342745Z","iopub.status.idle":"2023-09-15T03:16:51.276629Z","shell.execute_reply.started":"2023-09-15T03:16:49.342688Z","shell.execute_reply":"2023-09-15T03:16:51.275051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forests\n\nNext, I am using the Random Forest model which is an ensemble technique that depends on a collection of decision trees for making predictions. It operates as a non-parametric model, making no presumptions about data distributions. This algorithm trains multiple decision trees, each on a distinct sample of the initial training dataset achieved through resampling, and then combines their predictions to derive the outcome. This process is known as bagging (bootstrap aggregation). Random Forest is popular for delivering accurate predictions and handle datasets with a large number of features efficiently.","metadata":{}},{"cell_type":"code","source":"# setting the parameters for the model\nmodel_rf = RandomForestClassifier(n_estimators=500 , oob_score = True, n_jobs = -1,\n                                  random_state =50,max_leaf_nodes = 30)\n# fitting the model\nmodel_rf.fit(X_train, y_train)\n\n# make predictions\nprediction_test = model_rf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:51.278684Z","iopub.execute_input":"2023-09-15T03:16:51.279602Z","iopub.status.idle":"2023-09-15T03:16:54.006540Z","shell.execute_reply.started":"2023-09-15T03:16:51.279543Z","shell.execute_reply":"2023-09-15T03:16:54.005003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# measure the model performance\nmodel_performance(y_test,prediction_test, 'Random Forest')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:54.007999Z","iopub.execute_input":"2023-09-15T03:16:54.008473Z","iopub.status.idle":"2023-09-15T03:16:54.436611Z","shell.execute_reply.started":"2023-09-15T03:16:54.008429Z","shell.execute_reply":"2023-09-15T03:16:54.435237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_results = cross_validate(model_rf, X_train, y_train, cv=5)\nscores = cv_results[\"test_score\"]\nprint(\n    \"The mean cross-validation accuracy for Random Forests Model is: \"\n    f\"{scores.mean():.3f} ± {scores.std():.3f}\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:16:54.438012Z","iopub.execute_input":"2023-09-15T03:16:54.438455Z","iopub.status.idle":"2023-09-15T03:17:07.251814Z","shell.execute_reply.started":"2023-09-15T03:16:54.438421Z","shell.execute_reply":"2023-09-15T03:17:07.250539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of the model is very close to 80%, however, the mean cross-validation accuracy is higher than the logistic regression (LR) model. The model has yielded less number of False positives compared to the LR model. However, it is possible that we can further improve this model by tuning the hyperparameter and getting more accurate results. \n\nNext I will move to the survval analysis models.","metadata":{}},{"cell_type":"markdown","source":"## Survival Analysis\n\nThe machine learning models we've used to predict customer churn can inform us about which customers are likely to churn, but they don't provide insight into when this event will occur. In contrast, Survival Analysis offers a powerful solution by not only predicting the likelihood of customers discontinuing their business but also estimating when that event is likely to happen. \n\n\nSurvival Analysis contains a range of statistical methods designed to model the time until an event occurs, such as how long it takes for customers to churn. These models are useful when dealing with censored data, where some training data only provides partial observations. In our dataset, some customers have not yet churned within the observed time frame, making their future churn times uncertain. This type of data is referred to as censored, as it lacks complete information beyond the observation point.\n\nSurvival models rely on a survival prediction function, which can take the form of either a survival function or a cumulative hazard function. The survival function quantifies the likelihood that the event of interest (e.g., customer churn) has not happened up to a given time 't,' essentially representing the probability of survival beyond that time point. On the other hand, the hazard function provides insights into the event's occurrence rate or intensity, shedding light on when and how the event is likely to happen.\n\nThere are a few survival models such as the COX semi-parametric model, Weibull (with and without covariates), and Survival Forest. I am only exploring a few, popular models here.\n\nFor this analysis, I will use the **lifelines** and **scikit-survival** libraries in Python. Alternatively, you can also use the **PySurvival** library. ","metadata":{}},{"cell_type":"code","source":"# install the lifeline library\n! pip install lifelines","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:07.253223Z","iopub.execute_input":"2023-09-15T03:17:07.253666Z","iopub.status.idle":"2023-09-15T03:17:21.398330Z","shell.execute_reply.started":"2023-09-15T03:17:07.253634Z","shell.execute_reply":"2023-09-15T03:17:21.396806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lifelines import KaplanMeierFitter\nfrom lifelines import CoxPHFitter","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:21.410108Z","iopub.execute_input":"2023-09-15T03:17:21.410589Z","iopub.status.idle":"2023-09-15T03:17:21.524713Z","shell.execute_reply.started":"2023-09-15T03:17:21.410551Z","shell.execute_reply":"2023-09-15T03:17:21.523440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting some features of the initial dataset to apply the Kaplan-Meier estimator\ninput_data = df[['SeniorCitizen', 'Partner', 'Dependents', 'PaperlessBilling','MonthlyCharges', 'Contract','tenure', 'Churn', 'PaymentMethod']]\ninput_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:21.526094Z","iopub.execute_input":"2023-09-15T03:17:21.526500Z","iopub.status.idle":"2023-09-15T03:17:21.547838Z","shell.execute_reply.started":"2023-09-15T03:17:21.526467Z","shell.execute_reply":"2023-09-15T03:17:21.546729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Kaplan-Meier estimator\n\nThe Kaplan-Meier estimator is a non-parametric statistical tool that estimates the survival function. It constructs a survival curve using a stepwise approach, with each step corresponding to specific time points at which one or more customers experienced the event of interest, like churn. Kaplan-Meier estimator is not suitable for making predictions into the future since it simply extends the last observed survival probability. However, it is useful for understanding the overall population data and characterizing the survival patterns.\n\n* T:  the time of event of interest\n* E: (Censoring)  binary event indicator, determine between censored (0) and uncensored (1) observations.","metadata":{}},{"cell_type":"code","source":"# select T and E from the input data\nT = input_data['tenure']\nE = input_data['Churn']\n\n#fit them to the kmf model\nkmf = KaplanMeierFitter()\nkmf.fit(T, E)\n\nkmf.survival_function_\nkmf.cumulative_density_\nkmf.plot_survival_function()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:21.549769Z","iopub.execute_input":"2023-09-15T03:17:21.550236Z","iopub.status.idle":"2023-09-15T03:17:21.907972Z","shell.execute_reply.started":"2023-09-15T03:17:21.550191Z","shell.execute_reply":"2023-09-15T03:17:21.907127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The survival curve of the KM estimate represents each time point and  the probability of surviving the event (e.g., Churn event). For instance, around 40 months, there is a 70% probability that the customers will remain with the company, and there is a 30% probability that they leave. \n\nThe shaded region around the KM estimate curve represents the confidence interval, which tells how certain we can be that the true probability is within the shaded region.\n\nKM estimate is useful for gaining insights into the subgroups within the population. ","metadata":{}},{"cell_type":"code","source":"# KM estimates based on the contract type\nax = plt.subplot(111)\n\nfor name, grouped_df in input_data.groupby('Contract'):\n    kmf.fit(grouped_df[\"tenure\"], grouped_df[\"Churn\"], label=name)\n    kmf.plot_survival_function(ax=ax)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:21.909289Z","iopub.execute_input":"2023-09-15T03:17:21.910292Z","iopub.status.idle":"2023-09-15T03:17:22.341918Z","shell.execute_reply.started":"2023-09-15T03:17:21.910260Z","shell.execute_reply":"2023-09-15T03:17:22.340572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot indicates a notable trend: customers with month-to-month contracts shows a greater likelihood of churning compared to those with one or two-year contracts, aligning with real-world observations.","metadata":{}},{"cell_type":"code","source":"# KM estimates based on the contract type\nax = plt.subplot(111)\n\nfor name, grouped_df in input_data.groupby('PaymentMethod'):\n    kmf.fit(grouped_df[\"tenure\"], grouped_df[\"Churn\"], label=name)\n    kmf.plot_survival_function(ax=ax)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:22.343231Z","iopub.execute_input":"2023-09-15T03:17:22.343576Z","iopub.status.idle":"2023-09-15T03:17:22.851931Z","shell.execute_reply.started":"2023-09-15T03:17:22.343548Z","shell.execute_reply":"2023-09-15T03:17:22.850596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, customers who use electronic checks to pay the bills have a higher probability of churning than customers who make automatic payments. \n\nNow let's move to more advanced methods in survival analysis.","metadata":{}},{"cell_type":"markdown","source":"###  Cox’s proportional hazard’s model\n\nCox's Proportional Hazards Model is a widely used survival regression model that estimates the influence of individual variables on survival outcomes. It represents the hazard rate as a function of time and various covariates. Hence, enables us to quantify how each predictor affects the risk of an event occurring over time while maintaining the assumption that hazard ratios remain proportional. ","metadata":{}},{"cell_type":"code","source":"# let's use the same set of independent variables we used before with the ML model\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, stratify = y, test_size=0.2, random_state=33)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:22.853355Z","iopub.execute_input":"2023-09-15T03:17:22.853701Z","iopub.status.idle":"2023-09-15T03:17:22.865902Z","shell.execute_reply.started":"2023-09-15T03:17:22.853671Z","shell.execute_reply":"2023-09-15T03:17:22.864889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding back the target variable\nX_tr.loc[:,'Churn'] = y_tr","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:22.867402Z","iopub.execute_input":"2023-09-15T03:17:22.867717Z","iopub.status.idle":"2023-09-15T03:17:22.874315Z","shell.execute_reply.started":"2023-09-15T03:17:22.867690Z","shell.execute_reply":"2023-09-15T03:17:22.873189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting the Cox's Proportional\ncph = CoxPHFitter()\ncph.fit(X_tr, duration_col='tenure', event_col='Churn')\n\n# print the model summary\ncph.print_summary() ","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:22.875690Z","iopub.execute_input":"2023-09-15T03:17:22.876479Z","iopub.status.idle":"2023-09-15T03:17:23.425725Z","shell.execute_reply.started":"2023-09-15T03:17:22.876440Z","shell.execute_reply":"2023-09-15T03:17:23.424518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try to inerpret the model summary by using some of the above information.\n\n* **p-value** : The p-value associated with each coefficient assesses the statistical significance of that variable's impact on survival. Lower p-values (typically less than 0.05) suggest that the variable is likely to be statistically significant, indicating that it has a meaningful influence on survival. \n\n--According to the above summary 'contract', 'PaperlessBilling', and 'Partner' has lower-\n\n* **Hazard Ratio (exp(coef))**: The hazard ratio is derived from the exponentiated coefficients (exp(coef)). It tells you how a one-unit change in the predictor affects the hazard rate. A hazard ratio greater than 1 implies an increased hazard, while a hazard ratio less than 1 suggests a decreased hazard. For example, a hazard ratio of 2 means that the hazard doubles with each one-unit increase in the predictor.\n\n* **Concordance Index (C-index)**: The C-index is a measure of the model's predictive accuracy. It assesses how well the model discriminates between individuals who experienced the event and those who did not. A C-index of 0.5 represents random chance, while higher values (closer to 1) indicate better predictive performance. Since our model achive 0.82 C-Index, we can say that it fairly performs well. ","metadata":{}},{"cell_type":"code","source":"cph.plot()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:23.427335Z","iopub.execute_input":"2023-09-15T03:17:23.428259Z","iopub.status.idle":"2023-09-15T03:17:23.707688Z","shell.execute_reply.started":"2023-09-15T03:17:23.428216Z","shell.execute_reply":"2023-09-15T03:17:23.706544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cph plot assesses whether the hazard ratios for the covariate(s) being examined remain relatively constant over time. If the lines or curves for different levels of the covariate stay close to the reference line (HR = 1), it suggests that the proportional hazards assumption is met. However, if the lines cross or diverge significantly, it indicates a violation of the assumption, implying that the covariate's effect on the hazard rate varies over time.\n\nFrom the above plot, we can see that 'Contract_one year', Contract_two year, and 'Partner' coeeficients are aways from the reference line. Another way to check the deviations of the hazards assumption is to use the check_assumptions method.\n\n```python\ncph.check_assumptions(X_tr, show_plots=True, p_value_threshold=0.05)\n```\nThere are various methods to penalize Cox models and improve the model. I will not be exploring those here. For more details, see : https://scikit-survival.readthedocs.io/en/stable/user_guide/coxnet.html \n\nNext, let's look at the prediction results of our Cox's Proportional Hazards Model.\n","metadata":{}},{"cell_type":"code","source":"# add the churn to testing features\nX_te.loc[:,'Churn'] = y_te\n\n# select the customers who has not churned yet for predicitng the probabilities\nX_sel=X_te.loc[X_te['Churn']==0]","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:23.709083Z","iopub.execute_input":"2023-09-15T03:17:23.709457Z","iopub.status.idle":"2023-09-15T03:17:23.719452Z","shell.execute_reply.started":"2023-09-15T03:17:23.709425Z","shell.execute_reply":"2023-09-15T03:17:23.717862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the survival function for individuals, given their covariates.\ncph.predict_survival_function(X_sel)\n# return the predicted median survival time for each individual in your dataset.\ncph.predict_median(X_sel)\n# returns the partial hazard for the individuals, partial since the baseline hazard is not included\ncph.predict_partial_hazard(X_sel)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:23.721420Z","iopub.execute_input":"2023-09-15T03:17:23.721834Z","iopub.status.idle":"2023-09-15T03:17:23.848490Z","shell.execute_reply.started":"2023-09-15T03:17:23.721802Z","shell.execute_reply":"2023-09-15T03:17:23.847091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute the expected lifetime, 𝐸[𝑇] using covariates X\nX_pred=X_sel.copy()\nX_pred.loc[:,'Predicted tenure'] = cph.predict_expectation(X_sel)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:23.850743Z","iopub.execute_input":"2023-09-15T03:17:23.851064Z","iopub.status.idle":"2023-09-15T03:17:23.871791Z","shell.execute_reply.started":"2023-09-15T03:17:23.851037Z","shell.execute_reply":"2023-09-15T03:17:23.870549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample of a predictions\nX_pred.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:23.872946Z","iopub.execute_input":"2023-09-15T03:17:23.873299Z","iopub.status.idle":"2023-09-15T03:17:23.890237Z","shell.execute_reply.started":"2023-09-15T03:17:23.873271Z","shell.execute_reply":"2023-09-15T03:17:23.889066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample of the survival curves, based on the probability predictions\n# x-axis shows the time while y-axis shows the probabilities\ncph.predict_survival_function(X_sel.head(10)).plot()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:23.892331Z","iopub.execute_input":"2023-09-15T03:17:23.893428Z","iopub.status.idle":"2023-09-15T03:17:24.493305Z","shell.execute_reply.started":"2023-09-15T03:17:23.893384Z","shell.execute_reply":"2023-09-15T03:17:24.492296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, I want to look at some of the individual prediction results and combine the insights we got from the KM estimate.","metadata":{}},{"cell_type":"code","source":"X_pred.loc[(X_pred['Contract_One year'] == 0) & (X_pred['Contract_Two year'] == 0)].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:24.494774Z","iopub.execute_input":"2023-09-15T03:17:24.495095Z","iopub.status.idle":"2023-09-15T03:17:24.515364Z","shell.execute_reply.started":"2023-09-15T03:17:24.495069Z","shell.execute_reply":"2023-09-15T03:17:24.513930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pred.loc[(X_pred['Contract_One year'] == 1) | (X_pred['Contract_Two year'] == 1)].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:24.519109Z","iopub.execute_input":"2023-09-15T03:17:24.519472Z","iopub.status.idle":"2023-09-15T03:17:24.536721Z","shell.execute_reply.started":"2023-09-15T03:17:24.519443Z","shell.execute_reply":"2023-09-15T03:17:24.535407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The observations indicate the following:\n1. Customers with one-year or two-year contracts show higher probabilities, suggesting longer tenures (lifetime) with the company.\n2. Customers on monthly plans have lower probabilities, suggesting shorter lifetimes with the company.\n\nThese findings align with the survival function estimates we derived for the overall population using the Kaplan-Meier estimator.","metadata":{}},{"cell_type":"markdown","source":"### Random Survival Forests\n\nThe Random Survival Forest is similar to the Random Forest ensemble approach and utilises tree-based learners. As previously mentioned, it builds upon choosing different bootstrap samples to create a different tree from the original data. In each decision node, a subset of features is chosen at random to assess the splitting criteria and thresholds. The final prediction is derived by aggregating the predictions from all the individual trees in the ensemble. \n\nScikit-survival is a Python library for survival analysis that extends scikit-learn to include survival analysis algorithms and tools. First, let's install the library.","metadata":{}},{"cell_type":"code","source":"# install scikit-survival library\n!pip install scikit-survival","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:24.538060Z","iopub.execute_input":"2023-09-15T03:17:24.538486Z","iopub.status.idle":"2023-09-15T03:17:38.752663Z","shell.execute_reply.started":"2023-09-15T03:17:24.538458Z","shell.execute_reply":"2023-09-15T03:17:38.751136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sksurv.datasets import load_gbsg2\nfrom sksurv.preprocessing import OneHotEncoder\nfrom sksurv.ensemble import RandomSurvivalForest\nfrom sksurv.util import Surv","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:38.754349Z","iopub.execute_input":"2023-09-15T03:17:38.754727Z","iopub.status.idle":"2023-09-15T03:17:38.807312Z","shell.execute_reply.started":"2023-09-15T03:17:38.754694Z","shell.execute_reply":"2023-09-15T03:17:38.806324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I am using the same data set used before with the selected features\nall_features_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:38.810071Z","iopub.execute_input":"2023-09-15T03:17:38.810488Z","iopub.status.idle":"2023-09-15T03:17:38.829092Z","shell.execute_reply.started":"2023-09-15T03:17:38.810458Z","shell.execute_reply":"2023-09-15T03:17:38.827829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seperating training and testing data, bit different from the previous since we drop both Churn (E) and Tenure (T) from X\nX = all_features_df.drop(['Churn','tenure'],axis=1)\ny = all_features_df[['Churn', 'tenure']]\n\nrandom_state = 20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:38.830707Z","iopub.execute_input":"2023-09-15T03:17:38.831211Z","iopub.status.idle":"2023-09-15T03:17:38.845329Z","shell.execute_reply.started":"2023-09-15T03:17:38.831145Z","shell.execute_reply":"2023-09-15T03:17:38.844329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the model requires both T and E values as input to y\ny_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:38.847010Z","iopub.execute_input":"2023-09-15T03:17:38.847413Z","iopub.status.idle":"2023-09-15T03:17:38.860008Z","shell.execute_reply.started":"2023-09-15T03:17:38.847382Z","shell.execute_reply":"2023-09-15T03:17:38.858847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create structured array from data frame, we need to convert y dataframe into structured array\nnew_y_arr = Surv.from_dataframe('Churn', 'tenure', y_train)\nnew_y_arr","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:38.861536Z","iopub.execute_input":"2023-09-15T03:17:38.861912Z","iopub.status.idle":"2023-09-15T03:17:38.874591Z","shell.execute_reply.started":"2023-09-15T03:17:38.861882Z","shell.execute_reply":"2023-09-15T03:17:38.873482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the aparameters for the model\nrandom_state = 20\nrsf = RandomSurvivalForest(\n    n_estimators=1000, min_samples_split=10, min_samples_leaf=15, n_jobs=-1, random_state=random_state\n)\n# fit the model\nrsf.fit(X_train, new_y_arr)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:38.875980Z","iopub.execute_input":"2023-09-15T03:17:38.876395Z","iopub.status.idle":"2023-09-15T03:17:50.894536Z","shell.execute_reply.started":"2023-09-15T03:17:38.876363Z","shell.execute_reply":"2023-09-15T03:17:50.893178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can check how well the model performs by evaluating it on the test data. ","metadata":{}},{"cell_type":"code","source":"# format the testing y data\ny_test_arr = Surv.from_dataframe('Churn', 'tenure', y_test)\n\n# measure the C-Index \nrsf.score(X_test, y_test_arr)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:50.895992Z","iopub.execute_input":"2023-09-15T03:17:50.896350Z","iopub.status.idle":"2023-09-15T03:17:51.528285Z","shell.execute_reply.started":"2023-09-15T03:17:50.896320Z","shell.execute_reply":"2023-09-15T03:17:51.527234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **concordance index (CI)** is closer to 0.82, which is a good value. The survival regression model based on Cox's Proportional Hazards and the survival forest have similar CIs.  \n\n\nWhen making predictions, a new sample is run down the tree until it reaches a terminal node. Each terminal node non-parametrically estimates the survival and cumulative hazard function using the Kaplan-Meier and Nelson-Aalen estimators, respectively. The method also calculates a risk score that measures the expected number of events for one particular terminal node.","metadata":{}},{"cell_type":"code","source":"y_test.reset_index().drop(['index'], axis=1, inplace= True)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:51.529787Z","iopub.execute_input":"2023-09-15T03:17:51.530309Z","iopub.status.idle":"2023-09-15T03:17:51.538671Z","shell.execute_reply.started":"2023-09-15T03:17:51.530267Z","shell.execute_reply":"2023-09-15T03:17:51.537263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate predictions of the risk scores\ny_test['pred_risk_scores']=rsf.predict(X_test)\n# concat predictions and the actual values\ny_test.loc[y_test['Churn']==0].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:51.540332Z","iopub.execute_input":"2023-09-15T03:17:51.540824Z","iopub.status.idle":"2023-09-15T03:17:52.168673Z","shell.execute_reply.started":"2023-09-15T03:17:51.540788Z","shell.execute_reply":"2023-09-15T03:17:52.167633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us look at the predicted survival probabilities based on the characteristics of each customer. Todo so, I am going to combine the features and the T, E and the predicted risk scores. ","metadata":{}},{"cell_type":"code","source":"# reset index\nX_test.reset_index().drop(['index'], axis=1, inplace= True)\n# concat the testing data\nX_all = pd.concat([X_test, y_test], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:52.170048Z","iopub.execute_input":"2023-09-15T03:17:52.170395Z","iopub.status.idle":"2023-09-15T03:17:52.179130Z","shell.execute_reply.started":"2023-09-15T03:17:52.170366Z","shell.execute_reply":"2023-09-15T03:17:52.178028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly_contr =X_all.loc[(X_all['Contract_One year'] == 0) & (X_all['Contract_Two year'] == 0) & (X_all['Churn'] == 0)]\nmonthly_contr.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:52.180660Z","iopub.execute_input":"2023-09-15T03:17:52.180966Z","iopub.status.idle":"2023-09-15T03:17:52.204960Z","shell.execute_reply.started":"2023-09-15T03:17:52.180940Z","shell.execute_reply":"2023-09-15T03:17:52.203785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yearly_contr = X_all.loc[(X_all['Contract_One year'] == 1) | (X_all['Contract_Two year'] == 1) & (X_all['Churn'] == 0)]\nyearly_contr.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:52.206446Z","iopub.execute_input":"2023-09-15T03:17:52.206811Z","iopub.status.idle":"2023-09-15T03:17:52.230270Z","shell.execute_reply.started":"2023-09-15T03:17:52.206779Z","shell.execute_reply":"2023-09-15T03:17:52.229401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consistent with the outcomes from our survival regression and Kaplan-Meier estimation, we observe high risk scores among customers with month-to-month contracts, indicating a higher likelihood of churn. Conversely, customers committed to one or two-year contracts show lower risk scores, suggesting a lower probability of churn over the given time period.\n\nWe can gain more insights by looking at the predicted survival function of the customer data.","metadata":{}},{"cell_type":"code","source":"# seperate monthly and yearly contract of test features\nX_test_monthly = monthly_contr[X_test.columns]\nX_test_yearly = yearly_contr[X_test.columns]","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:52.231861Z","iopub.execute_input":"2023-09-15T03:17:52.232730Z","iopub.status.idle":"2023-09-15T03:17:52.239837Z","shell.execute_reply.started":"2023-09-15T03:17:52.232697Z","shell.execute_reply":"2023-09-15T03:17:52.238469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate the Survival Probability plot\ndef survival_probability_plot(surv, contract):\n    for i, s in enumerate(surv):\n        plt.step(rsf.unique_times_, s, where=\"post\", label=str(i))\n    plt.ylabel(\"Survival probability\")\n    plt.xlabel(\"Time in Months\")\n    plt.title('Survival probabilities of cutomers on: ' + contract)\n    plt.legend()\n    plt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:52.241127Z","iopub.execute_input":"2023-09-15T03:17:52.241530Z","iopub.status.idle":"2023-09-15T03:17:52.255197Z","shell.execute_reply.started":"2023-09-15T03:17:52.241499Z","shell.execute_reply":"2023-09-15T03:17:52.253984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select a sample of the test data (only customers on monthly contract) to visulize the survival probabilities\nsurv_monthly_contract = rsf.predict_survival_function(X_test_monthly.sample(n=5), return_array=True)\nsurvival_probability_plot(surv_monthly_contract, 'Month-to-month')","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:52.256951Z","iopub.execute_input":"2023-09-15T03:17:52.257688Z","iopub.status.idle":"2023-09-15T03:17:52.844770Z","shell.execute_reply.started":"2023-09-15T03:17:52.257655Z","shell.execute_reply":"2023-09-15T03:17:52.843524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select a sample of the test data to visulize the survival probabilities\nsurv_monthly_contract = rsf.predict_survival_function(X_test_yearly.sample(n=5), return_array=True)\nsurvival_probability_plot(surv_monthly_contract, \"One or Two years\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:52.846066Z","iopub.execute_input":"2023-09-15T03:17:52.846402Z","iopub.status.idle":"2023-09-15T03:17:53.437267Z","shell.execute_reply.started":"2023-09-15T03:17:52.846375Z","shell.execute_reply":"2023-09-15T03:17:53.436094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The survival probability plot shows the estimated survival probability and the time points at which survival probabilities are estimated. Consistent, with our previous conclusions, customers who are on a one-year or two-year contract has higher survival probabilities than for the customers on monthly contract from churning.","metadata":{}},{"cell_type":"markdown","source":"#### Permutation based feature importance\n\nPermutation feature importance is a model inspection technique. The scikit-learn toolbox provides a method to determine the permutation feature importance of the features used in the survival forest model. See https://scikit-learn.org/stable/modules/permutation_importance.html for more details.","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:53.439265Z","iopub.execute_input":"2023-09-15T03:17:53.440079Z","iopub.status.idle":"2023-09-15T03:17:53.445907Z","shell.execute_reply.started":"2023-09-15T03:17:53.440035Z","shell.execute_reply":"2023-09-15T03:17:53.444778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# measure the feature importance \nresult = permutation_importance(rsf, X_test, y_test_arr, n_repeats=15, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:17:53.447643Z","iopub.execute_input":"2023-09-15T03:17:53.448385Z","iopub.status.idle":"2023-09-15T03:19:02.701767Z","shell.execute_reply.started":"2023-09-15T03:17:53.448345Z","shell.execute_reply":"2023-09-15T03:19:02.700775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# meand and the std values of the feature importance\npd.DataFrame(\n    {\n        k: result[k]\n        for k in (\n            \"importances_mean\",\n            \"importances_std\",\n        )\n    },\n    index=X_test.columns,\n).sort_values(by=\"importances_mean\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T03:19:02.703313Z","iopub.execute_input":"2023-09-15T03:19:02.703621Z","iopub.status.idle":"2023-09-15T03:19:02.716968Z","shell.execute_reply.started":"2023-09-15T03:19:02.703594Z","shell.execute_reply":"2023-09-15T03:19:02.715702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results show that the most important features of the model are the contract type and the monthly charges. There is still room for this model. I have not done any hyperparameter tuning with the model, and it may be a good starting point to improve the model further.","metadata":{}},{"cell_type":"markdown","source":"### Conclusions\n\n* I have applied both machine learning and survival analysis techniques to tackle the customer churn prediction challenge. While all the models have indicated good performance, ensemble methods like random forests and survival random forests offer better accuracy and efficiency.\n* ML models primarily focus on identifying which customers are likely to churn within a given pool of customers. \n* In contrast, survival analysis addresses the question of when a customer is likely to churn, offering probabilities of customer survival over specific time periods.\n* The choice between these two approaches depends on the nature of the customer churn issue a company is dealing with. By understanding \"who\" is likely to churn through ML or \"when\" they are likely to churn through survival analysis, companies can change or modify their retention strategies more effectively and take proactive measures to extend their customers' relationships.","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}